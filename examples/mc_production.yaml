id: <your id here>
workdir: <your working directory here>
## optional: to automatically update monitoring plots
plotdir: <your plotting directory here>

## directory for output files
stageout location: <your stageout directory here>

## optional: chirp server to use for stage-in / stage-out
# chirp server: "earth.crc.nd.edu:9094"
# chirp root: /hadoop/store/user/

## optional: XrootD for streaming local datasets
# xrootd server: "ndcms.crc.nd.edu"
# xrootd root: /hadoop

## optional: SRM for stage-out
# srm url: "srm://ndcms.crc.nd.edu:8443/srm/v2/server?SFN=/hadoop/"
# srm root: /hadoop

## report jobs to the CMS dashboard
use dashboard: true

## merge output files to this size
merge size: 3.5G

## optional: specify directories in CMSSW work area to omit from sandbox
# sandbox blacklist: ['*DrawPlots*']

advanced:
  ## start killing jobs with excessive runtimes after this many successful
  ## jobs
  abort threshold: 10
  ## define excessive runtime in multiples of the average runtime
  # abort multiplier: 4
  ## override automatic proxy renewal
  # renew proxy: false
  ## level of verbosity.  Everything is 1, only critical messages is 5, default 2
  log level: 0
  ## how many jobs to create and keep in the queue
  # payload: 400

## DBS instance to publish to
# dbs instance: phys03
## overwrite the username to use for publishing, in case it is different from the environment var $USER
# publish user: username
## specify sandbox instead of creating one at startup
# recycle sandbox: path/to/sandbox.tar.bz2

task defaults:
    cmssw config: mc_production.py
    ## if this is not specified, the cmssw config will be loaded to look for output modules and global tags (slower)
    outputs: [minbias.root]
    ## optional renaming of files, based on their basename and extension:
    # output format: "this_is_file_{id}_{base}.{ext}"
    publish label: 'test'
    ## default job size
    # lumis per job: 25

tasks:
  - {label: minbias, events per job: 225, num jobs: 10000}

